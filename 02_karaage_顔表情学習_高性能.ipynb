{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mag381/mukonoso/blob/main/02_karaage_%E9%A1%94%E8%A1%A8%E6%83%85%E5%AD%A6%E7%BF%92_%E9%AB%98%E6%80%A7%E8%83%BD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ye3dTvNpt3D"
      },
      "source": [
        "# AIモデルの性能を性能を上げる方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNgUsGFZBLEh"
      },
      "source": [
        "## 教師データのダウンロード\n",
        "\n",
        "ジャンケンの手の形の教師データをGitHubからダウンロード（Clone）します。\n",
        "\n",
        "2,3行目はダウンロードしたデータから、使用するデータ以外の不要なファイルを削除しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JqHcRAEB2n_"
      },
      "source": [
        "#!git clone https://github.com/karaage0703/janken_dataset datasets\n",
        "#!rm -rf /content/datasets/.git\n",
        "#!rm /content/datasets/LICENSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "f5eyW1ym6iaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "QXNctfzd6rD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWdaU-dW4kvI"
      },
      "source": [
        "## 教師データを訓練データ（Train Data）とテストデータ（Validation Data）に分ける"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmdetFKFvWp"
      },
      "source": [
        "教師データのディレクトリと、ターゲットとなるディレクトリ（この下に訓練データのディレクトリと検証データのディレクトリが生成される）を指定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA4SGY54GY6b"
      },
      "source": [
        "dataset_original_dir = 'datasets'\n",
        "dataset_root_dir = 'target_datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kevX13wW47Ub"
      },
      "source": [
        "教師データを訓練データのディレクトリ(train)と検証データのディレクトリ（val）に分割するスクリプトをダウンロードします。\n",
        "\n",
        "スクリプトのプログラムに関しては、本ノートブックの主題では無いので割愛します。興味ある方は以下のアドレスで、ソフトの中身を確認して下さい。\n",
        "\n",
        "https://raw.githubusercontent.com/karaage0703/karaage-ai-book/master/util/split_train_val.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGSRbbuIreaQ"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/karaage0703/karaage-ai-book/master/util/split_train_val.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMHcDDbJrvQu"
      },
      "source": [
        "import split_train_val\n",
        "split_train_val.image_dir_train_val_split(dataset_original_dir, dataset_root_dir, train_size=0.67)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3PHCUYZJd-W"
      },
      "source": [
        "train_dir = 'target_datasets/train'\n",
        "val_dir = 'target_datasets/val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roHDM7MB1w9S"
      },
      "source": [
        "## ラベルファイルの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSq7-H5b2DVx"
      },
      "source": [
        "学習するファイルのラベルを作成します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN5gSnfyz7Tp"
      },
      "source": [
        "必要なライブラリをインポートします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQkx3zp1Pwt"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjsFU39e3ncO"
      },
      "source": [
        "データを保存する場所を指定します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UhMCgO53m4M"
      },
      "source": [
        "backup_dir = './model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKkS5ed_Jx6c"
      },
      "source": [
        "ラベルデータを作成します（最後に表示される class numberが画像の種類の数です）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRk9ZAWZlj9C"
      },
      "source": [
        "labels = [d for d in os.listdir(dataset_original_dir) \\\n",
        "    if os.path.isdir(os.path.join(dataset_original_dir, d))]\n",
        "labels.sort()\n",
        "\n",
        "if os.path.exists(backup_dir):\n",
        "  shutil.rmtree(backup_dir)\n",
        "\n",
        "os.makedirs(backup_dir)\n",
        "\n",
        "with open(backup_dir + '/labels.txt','w') as f:\n",
        "  for label in labels:\n",
        "    f.write(label+\"\\n\")\n",
        "\n",
        "NUM_CLASSES = len(labels)\n",
        "print(\"class number=\" + str(NUM_CLASSES))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzE-lgUNJ3e8"
      },
      "source": [
        "ラベルを確認します。ラベル名（choki, gu, pa）が並んでいればOKです"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRBOzF1i6HnT"
      },
      "source": [
        "!cat ./model/labels.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLcN1JHv2Q6g"
      },
      "source": [
        "## 学習の事前準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zfpKvOOy6nM"
      },
      "source": [
        "### ライブラリのインポート"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln1wU_8DFe22"
      },
      "source": [
        "必要なライブラリをインポートします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ltMN9XGBqSa"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDmuYdo-BnJK"
      },
      "source": [
        "続いて、他に必要なライブラリをインストールします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvUS5Z5CEjQp"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwz5rPa333ch"
      },
      "source": [
        "先ほど作成したラベルファイルから、ラベル情報を読み込みます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkbhFC1Dxrb-"
      },
      "source": [
        "labels = []\n",
        "with open(backup_dir + '/labels.txt','r') as f:\n",
        "    for line in f:\n",
        "        labels.append(line.rstrip())\n",
        "print(labels)\n",
        "\n",
        "NUM_CLASSES = len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsWHDcihy9in"
      },
      "source": [
        "### 学習のハイパーパラメータの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E3MR7dzTddI"
      },
      "source": [
        "学習のハイパーパラメータの設定をします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBSQYa3QE9jQ"
      },
      "source": [
        "# 学習率\n",
        "LEARNING_RATE = 0.001\n",
        "# エポック（世代数）\n",
        "EPOCHS = 20\n",
        "# バッチサイズ\n",
        "BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQl_DBFozCom"
      },
      "source": [
        "### データセットの前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRKlr-AcTVEn"
      },
      "source": [
        "データをTensorFlowで扱える形式に変換します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhoQwaPTEdTk"
      },
      "source": [
        "IMAGE_SIZE = 64\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "val_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_data_gen.flow_from_directory(\n",
        "    train_dir, target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    color_mode='rgb', batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True)\n",
        "\n",
        "validation_data = val_data_gen.flow_from_directory(\n",
        "    val_dir, target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    color_mode='rgb', batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNdMltvlcfuZ"
      },
      "source": [
        "## モデルの変更"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vyJ2iagD5L"
      },
      "source": [
        "model_2 = Sequential()\n",
        "\n",
        "model_2.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Conv2D(32, (3, 3)))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_2.add(Dropout(0.25))\n",
        "\n",
        "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Conv2D(64, (3, 3)))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model_2.add(Dropout(0.25))\n",
        "\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(512))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "\n",
        "model_2.add(Dense(NUM_CLASSES))\n",
        "model_2.add(Activation('softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model_2.compile(opt, loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWF7ARdgMpv"
      },
      "source": [
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--VGKh1UgTlL"
      },
      "source": [
        "%%time\n",
        "history = model_2.fit(train_data, epochs=EPOCHS, validation_data=validation_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOhPpqyKgWLS"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EPM6GB9gnHN"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWYx-CQUlxbl"
      },
      "source": [
        "# Get the ordered list of class names:\n",
        "import PIL.Image as Image\n",
        "class_names = validation_data.class_indices.items()\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle = True\n",
        "validation_data.batch_size = BATCH_SIZE\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "validation_id = np.argmax(validation_label_batch, axis=-1)\n",
        "validation_label = class_names[validation_id]\n",
        "predicted_batch = model_2.predict(validation_image_batch)\n",
        "\n",
        "# Returns the indices of the maximum values along a given axis\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "\n",
        "# Return the maximum values along a given axis\n",
        "predicted_score = np.max(predicted_batch, axis=-1)\n",
        "\n",
        "predicted_label_batch = class_names[predicted_id]\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Display the classification results for the first 30 images\n",
        "for n in range(min(validation_image_batch.shape[0], 30)):\n",
        "  plt.subplot(6, 5, n + 1)\n",
        "\n",
        "  # Convert the range from -1 to 1 to the range from 0 to 1\n",
        "  plt.imshow(np.array(validation_image_batch[n]*255,np.int32))\n",
        "  color = 'green' if predicted_id[n] == validation_id[n] else 'red'\n",
        "  predicted_label = predicted_label_batch[n].title()\n",
        "  plt.title(predicted_label + ' ({:.2f}, {})'.format(\n",
        "      predicted_score[n], validation_label[n]), color=color)\n",
        "  plt.axis('off')\n",
        "\n",
        "_ = plt.suptitle('Model predictions (green: correct, red: incorrect)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J5EjjD8KqN5"
      },
      "source": [
        "## 転移学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxIdQFCC0phe"
      },
      "source": [
        "### MobileNetV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIvEAVditv49"
      },
      "source": [
        "MobileNet V2を読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "zZzS7YvI820S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdqBs1jUFkYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb34467-9c9f-49a1-fc3e-a731263b75eb"
      },
      "source": [
        "mobilenet_pretrained_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
        "    include_top=False,\n",
        "    weights= 'imagenet')\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-286a2c3e2f7a>:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  mobilenet_pretrained_model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V16vnQnoFiCC"
      },
      "source": [
        "mobilenet_pretrained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rptFRKO-MWjO"
      },
      "source": [
        "(image_data, _) = train_data.__next__()\n",
        "feature_batch = mobilenet_pretrained_model(image_data)\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ecTtxEjNGhF"
      },
      "source": [
        "mobilenet_pretrained_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXrXLygLNGxw"
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS7-Xi86u70j"
      },
      "source": [
        "注意： 以下の行`prediction_layer`に活性化層として`softmax`を指定していなかったため書籍では性能が正しく出ていませんでした。指定することで`mobilenet`でも高い性能が出ます。\n",
        "\n",
        "参考： https://github.com/karaage0703/karaage-ai-book/issues/26"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5_PMhi-O-Vs"
      },
      "source": [
        "prediction_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oan901gO-iS"
      },
      "source": [
        "mobilenet_train_model = tf.keras.Sequential([\n",
        "  mobilenet_pretrained_model,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EI2RuDHPnUh"
      },
      "source": [
        "mobilenet_train_model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqg484iuap9F"
      },
      "source": [
        "mobilenet_train_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JcMwTmsQwQ2"
      },
      "source": [
        "学習します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdtbcdHeQiIw"
      },
      "source": [
        "%%time\n",
        "history = mobilenet_train_model.fit(train_data, epochs=EPOCHS, validation_data=validation_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiCKUD0WQifp"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUOOVODmQiXy"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHGcOZ8NQidq"
      },
      "source": [
        "# Get the ordered list of class names:\n",
        "import PIL.Image as Image\n",
        "class_names = validation_data.class_indices.items()\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle = True\n",
        "validation_data.batch_size = BATCH_SIZE\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "validation_id = np.argmax(validation_label_batch, axis=-1)\n",
        "validation_label = class_names[validation_id]\n",
        "predicted_batch = mobilenet_train_model.predict(validation_image_batch)\n",
        "\n",
        "# Returns the indices of the maximum values along a given axis\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "\n",
        "# Return the maximum values along a given axis\n",
        "predicted_score = np.max(predicted_batch, axis=-1)\n",
        "\n",
        "predicted_label_batch = class_names[predicted_id]\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Display the classification results for the first 30 images\n",
        "for n in range(min(validation_image_batch.shape[0], 30)):\n",
        "  plt.subplot(6, 5, n + 1)\n",
        "\n",
        "  # Convert the range from -1 to 1 to the range from 0 to 1\n",
        "  plt.imshow(np.array(validation_image_batch[n]*255,np.int32))\n",
        "  color = 'green' if predicted_id[n] == validation_id[n] else 'red'\n",
        "  predicted_label = predicted_label_batch[n].title()\n",
        "  plt.title(predicted_label + ' ({:.2f}, {})'.format(\n",
        "      predicted_score[n], validation_label[n]), color=color)\n",
        "  plt.axis('off')\n",
        "\n",
        "_ = plt.suptitle('Model predictions (green: correct, red: incorrect)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEf7-ekp0HyD"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle =  False\n",
        "validation_data.batch_size = 1\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "predicted = mobilenet_train_model.predict(validation_data, steps=validation_data.n)\n",
        "predicted_classes = np.argmax(predicted, axis=-1)\n",
        "\n",
        "\n",
        "# Apply normalization\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "cm = confusion_matrix(validation_data.classes, predicted_classes)\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# https://matplotlib.org/users/colormaps.html\n",
        "sns.heatmap(cm, annot=True, square=True, cmap=plt.cm.Blues,\n",
        "            xticklabels=validation_data.class_indices,\n",
        "            yticklabels=validation_data.class_indices)\n",
        "\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.xlim([0.0, 3.0])\n",
        "plt.ylim([0.0, 3.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBFeUJ6WdgMr"
      },
      "source": [
        "### EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8K1V_1vQiTZ"
      },
      "source": [
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opeIBIjvQiO1"
      },
      "source": [
        "feature_extractor_url = \"https://tfhub.dev/google/efficientnet/b0/feature-vector/1\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEcw_2y1ePgE"
      },
      "source": [
        "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
        "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPNAVZicePqY"
      },
      "source": [
        "feature_extractor_layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras as keras"
      ],
      "metadata": {
        "id": "-tbk5FIJHPsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Fix issue https://github.com/karaage0703/karaage-ai-book/issues/42\n",
        "#opt = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "HFGJTwq3KAL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h7gP8of4Iw2w"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK2ZTnWeePvr"
      },
      "source": [
        "efficientnet_model = keras.Sequential([\n",
        "  feature_extractor_layer,\n",
        "  keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXtQkPNbePnk"
      },
      "source": [
        "efficientnet_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4SfaioIfIVa"
      },
      "source": [
        "efficientnet_model.compile ( opt,\n",
        "                           loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6URlxzgfIhE"
      },
      "source": [
        "%%time\n",
        "history = efficientnet_model.fit(train_data, epochs=EPOCHS, validation_data=validation_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-tOx69exTV2"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TWIGQ4bw5qk"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.ylim([0.0, 1.1])\n",
        "plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqE2OgdvfIo4"
      },
      "source": [
        "# Get the ordered list of class names:\n",
        "import PIL.Image as Image\n",
        "class_names = validation_data.class_indices.items()\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle = True\n",
        "validation_data.batch_size = BATCH_SIZE\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "validation_id = np.argmax(validation_label_batch, axis=-1)\n",
        "validation_label = class_names[validation_id]\n",
        "predicted_batch = efficientnet_model.predict(validation_image_batch)\n",
        "\n",
        "# Returns the indices of the maximum values along a given axis\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "\n",
        "# Return the maximum values along a given axis\n",
        "predicted_score = np.max(predicted_batch, axis=-1)\n",
        "\n",
        "predicted_label_batch = class_names[predicted_id]\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Display the classification results for the first 30 images\n",
        "for n in range(min(validation_image_batch.shape[0], 30)):\n",
        "  plt.subplot(6, 5, n + 1)\n",
        "\n",
        "  # Convert the range from -1 to 1 to the range from 0 to 1\n",
        "  plt.imshow(np.array(validation_image_batch[n]*255,np.int32))\n",
        "  color = 'green' if predicted_id[n] == validation_id[n] else 'red'\n",
        "  predicted_label = predicted_label_batch[n].title()\n",
        "  plt.title(predicted_label + ' ({:.2f}, {})'.format(\n",
        "      predicted_score[n], validation_label[n]), color=color)\n",
        "  plt.axis('off')\n",
        "\n",
        "_ = plt.suptitle('Model predictions (green: correct, red: incorrect)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPJidKuH1BRI"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle =  False\n",
        "validation_data.batch_size = 1\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "predicted = efficientnet_model.predict_generator(validation_data, steps=validation_data.n)\n",
        "predicted_classes = np.argmax(predicted, axis=-1)\n",
        "\n",
        "# Apply normalization\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "cm = confusion_matrix(validation_data.classes, predicted_classes)\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "\n",
        "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# https://matplotlib.org/users/colormaps.html\n",
        "sns.heatmap(cm, annot=True, square=True, cmap=plt.cm.Blues,\n",
        "            xticklabels=validation_data.class_indices,\n",
        "            yticklabels=validation_data.class_indices)\n",
        "\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.xlim([0.0, 3.0])\n",
        "plt.ylim([0.0, 3.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egFnlerS1VqM"
      },
      "source": [
        "## データ水増し"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vSbDD5S2HWN"
      },
      "source": [
        "データをデータ水増し用のディレクトリにコピー"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igCA1nx2diA"
      },
      "source": [
        "!cp -r ./target_datasets/train ./target_datasets/train_aug\n",
        "!cp ./datasets/angry/KA.AN1.39.jpg ./target_datasets/train_aug/angry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAXiExKY2pSM"
      },
      "source": [
        "データ水増し用のディレクトリを定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8AvOT762oe_"
      },
      "source": [
        "train_aug_dir = 'target_datasets/train_aug'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UWiC6692ZWc"
      },
      "source": [
        "必要なライブラリをインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJoiof6j1TR3"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCJthQDm1bWZ"
      },
      "source": [
        "# ヒストグラム均一化\n",
        "def equalizeHistRGB(src):\n",
        "    RGB = cv2.split(src)\n",
        "    Blue   = RGB[0]\n",
        "    Green = RGB[1]\n",
        "    Red    = RGB[2]\n",
        "    for i in range(3):\n",
        "        cv2.equalizeHist(RGB[i])\n",
        "\n",
        "    img_hist = cv2.merge([RGB[0],RGB[1], RGB[2]])\n",
        "    return img_hist\n",
        "\n",
        "# ガウシアンノイズ\n",
        "def addGaussianNoise(src):\n",
        "    row,col,ch= src.shape\n",
        "    mean = 0\n",
        "    var = 0.1\n",
        "    sigma = 15\n",
        "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
        "    gauss = gauss.reshape(row,col,ch)\n",
        "    noisy = src + gauss\n",
        "\n",
        "    return noisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_xx5wej1gEc"
      },
      "source": [
        "labels = os.listdir(dataset_original_dir)\n",
        "\n",
        "image_file_names = []\n",
        "\n",
        "for label in labels:\n",
        "  image_files = glob.glob(train_aug_dir + '/' + label +'/*')\n",
        "  for image_file in image_files:\n",
        "    image_file_names.append(image_file)\n",
        "\n",
        "print(image_file_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzZPrfcx1kY1"
      },
      "source": [
        "# ルックアップテーブルの生成\n",
        "min_table = 50\n",
        "max_table = 205\n",
        "diff_table = max_table - min_table\n",
        "gamma1 = 0.75\n",
        "gamma2 = 1.5\n",
        "\n",
        "LUT_HC = np.arange(256, dtype = 'uint8')\n",
        "LUT_LC = np.arange(256, dtype = 'uint8')\n",
        "LUT_G1 = np.arange(256, dtype = 'uint8')\n",
        "LUT_G2 = np.arange(256, dtype = 'uint8')\n",
        "\n",
        "LUTs = []\n",
        "\n",
        "# 平滑化用\n",
        "average_square = (10,10)\n",
        "\n",
        "# ハイコントラストLUT作成\n",
        "for i in range(0, min_table):\n",
        "  LUT_HC[i] = 0\n",
        "\n",
        "for i in range(min_table, max_table):\n",
        "  LUT_HC[i] = 255 * (i - min_table) / diff_table\n",
        "\n",
        "for i in range(max_table, 255):\n",
        "  LUT_HC[i] = 255\n",
        "\n",
        "# その他LUT作成\n",
        "for i in range(256):\n",
        "  LUT_LC[i] = min_table + i * (diff_table) / 255\n",
        "  LUT_G1[i] = 255 * pow(float(i) / 255, 1.0 / gamma1)\n",
        "  LUT_G2[i] = 255 * pow(float(i) / 255, 1.0 / gamma2)\n",
        "\n",
        "LUTs.append(LUT_HC)\n",
        "LUTs.append(LUT_LC)\n",
        "LUTs.append(LUT_G1)\n",
        "LUTs.append(LUT_G2)\n",
        "\n",
        "\n",
        "for image_file in image_file_names:\n",
        "#  print(image_file)\n",
        "  # 画像の読み込み\n",
        "  img_src = cv2.imread(image_file, 1)\n",
        "  trans_img = []\n",
        "  trans_img.append(img_src)\n",
        "\n",
        "  # LUT変換\n",
        "  for i, LUT in enumerate(LUTs):\n",
        "    trans_img.append(cv2.LUT(img_src, LUT))\n",
        "\n",
        "\t# 平滑化\n",
        "  trans_img.append(cv2.blur(img_src, average_square))\n",
        "\n",
        "\t# ヒストグラム均一化\n",
        "  trans_img.append(equalizeHistRGB(img_src))\n",
        "\n",
        "\t# ノイズ付加\n",
        "  trans_img.append(addGaussianNoise(img_src))\n",
        "\n",
        "\t# 反転\n",
        "  flip_img = []\n",
        "  for img in trans_img:\n",
        "     flip_img.append(cv2.flip(img, 1))\n",
        "  trans_img.extend(flip_img)\n",
        "\n",
        "  dir_name =  os.path.splitext(os.path.dirname(image_file))[0]\n",
        "  base_name =  os.path.splitext(os.path.basename(image_file))[0]\n",
        "  img_src.astype(np.float64)\n",
        "\n",
        "  for i, img in enumerate(trans_img):\n",
        "    if i > 0:\n",
        "      cv2.imwrite(dir_name + '/trans_' + base_name + '_' + str(i-1) + '.jpg' ,img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmvHPT-u2-Yi"
      },
      "source": [
        "ディレクトリの中身を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8nNzKIB1mbh"
      },
      "source": [
        "!ls target_datasets/train_aug/angry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQleQJY93LYN"
      },
      "source": [
        "水増しされたファイル(`trans_xxx.jpg`) というファイルが大量にあればOKです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LudQSDXI4S73"
      },
      "source": [
        "水増しされたデータを確認してみましょう。\n",
        "\n",
        "代表例として、冒頭確認したチョキの画像の水増しされた画像を結合して、1枚の写真にして表示します。\n",
        "\n",
        "エラーが出ますが、学習には影響しないので、無視して続けます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGwLnxmz4VrG"
      },
      "source": [
        "tmp_file_name = './target_datasets/train_aug/angry/trans_choki_01'\n",
        "\n",
        "tmp_image = []\n",
        "for i in range(16):\n",
        "      tmp_image.append(cv2.imread(tmp_file_name + '_' + str(i) + '.jpg'))\n",
        "\n",
        "tmp_hconcat_image = []\n",
        "tmp_hconcat_image.append(cv2.hconcat(tmp_image[0:4]))\n",
        "tmp_hconcat_image.append(cv2.hconcat(tmp_image[4:8]))\n",
        "tmp_hconcat_image.append(cv2.hconcat(tmp_image[8:12]))\n",
        "\n",
        "concat_image = cv2.vconcat(tmp_hconcat_image[0:4])\n",
        "concat_image = cv2.resize(concat_image, dsize=(640, 640))\n",
        "cv2.imwrite('concat.jpg', concat_image)\n",
        "\n",
        "from IPython.display import Image as IPImage\n",
        "from IPython.display import display_jpeg\n",
        "display_jpeg(IPImage('concat.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZpgXZDS34RQ"
      },
      "source": [
        "### 学習の事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcKkYcTm30OX"
      },
      "source": [
        "train_aug_data = train_data_gen.flow_from_directory(\n",
        "    train_aug_dir, target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    color_mode='rgb', batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6cwKayS4mDe"
      },
      "source": [
        "モデル作成に必要なライブラリのインポート"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3t0ZQ_i4lny"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "#from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiU5AKao4qN_"
      },
      "source": [
        "MNISTベースのモデルを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfCZw7wL3BVT"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(NUM_CLASSES))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
        "\n",
        "model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6UPpQLE49Bw"
      },
      "source": [
        "## AIモデルの学習\n",
        "AIモデルの学習を行います"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bsaUSRp45DV"
      },
      "source": [
        "%%time\n",
        "history = model.fit(train_aug_data, epochs=EPOCHS, validation_data=validation_data, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfr_37xy5O3X"
      },
      "source": [
        "## 学習結果の可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz76fgVu5Bci"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'val_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgPt9kGf5XBJ"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlim([0.0, EPOCHS])\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.legend(['acc', 'val_acc'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx3nDXqc5dKs"
      },
      "source": [
        "# Get the ordered list of class names:\n",
        "import PIL.Image as Image\n",
        "class_names = validation_data.class_indices.items()\n",
        "class_names = np.array([key.title() for key, value in class_names])\n",
        "\n",
        "validation_data.reset()\n",
        "validation_data.shuffle = True\n",
        "validation_data.batch_size = BATCH_SIZE\n",
        "\n",
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "\n",
        "validation_id = np.argmax(validation_label_batch, axis=-1)\n",
        "validation_label = class_names[validation_id]\n",
        "predicted_batch = model.predict(validation_image_batch)\n",
        "\n",
        "# Returns the indices of the maximum values along a given axis\n",
        "predicted_id = np.argmax(predicted_batch, axis=-1)\n",
        "\n",
        "# Return the maximum values along a given axis\n",
        "predicted_score = np.max(predicted_batch, axis=-1)\n",
        "\n",
        "predicted_label_batch = class_names[predicted_id]\n",
        "\n",
        "plt.figure(figsize=(16, 9))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Display the classification results for the first 30 images\n",
        "for n in range(min(validation_image_batch.shape[0], 30)):\n",
        "  plt.subplot(6, 5, n + 1)\n",
        "\n",
        "  # Convert the range from -1 to 1 to the range from 0 to 1\n",
        "  plt.imshow(np.array(validation_image_batch[n]*255,np.int32))\n",
        "  color = 'green' if predicted_id[n] == validation_id[n] else 'red'\n",
        "  predicted_label = predicted_label_batch[n].title()\n",
        "  plt.title(predicted_label + ' ({:.2f}, {})'.format(\n",
        "      predicted_score[n], validation_label[n]), color=color)\n",
        "  plt.axis('off')\n",
        "\n",
        "_ = plt.suptitle('Model predictions (green: correct, red: incorrect)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "0g4W8cDdZS2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data.reset()\n",
        "validation_data.shuffle =  False\n",
        "validation_data.batch_size = 1"
      ],
      "metadata": {
        "id": "Gxf_4trSZVYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the first batch from the validation data\n",
        "#for validation_image_batch, validation_label_batch in validation_data:\n",
        "  #break\n",
        "#predicted = model.predict_generator(validation_data, steps=validation_data.n)\n",
        "#predicted_classes = np.argmax(predicted, axis=-1)"
      ],
      "metadata": {
        "id": "HYzKuBsjZlEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the first batch from the validation data\n",
        "for validation_image_batch, validation_label_batch in validation_data:\n",
        "  break\n",
        "predicted = model.predict(validation_data, steps=validation_data.n)\n",
        "predicted_classes = np.argmax(predicted, axis=-1)"
      ],
      "metadata": {
        "id": "MPsSqEthZXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply normalization\n",
        "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "cm = confusion_matrix(validation_data.classes, predicted_classes)\n",
        "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "plt.figure(figsize=(12, 9))"
      ],
      "metadata": {
        "id": "wOGIS08zZe4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQZhX5D95p42"
      },
      "source": [
        "# https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "# https://matplotlib.org/users/colormaps.html\n",
        "sns.heatmap(cm, annot=True, square=True, cmap=plt.cm.Blues,\n",
        "            xticklabels=validation_data.class_indices,\n",
        "            yticklabels=validation_data.class_indices)\n",
        "\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.xlim([0.0, 3.0])\n",
        "plt.ylim([0.0, 3.0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZef8mEtxf3P"
      },
      "source": [
        "## モデル保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSV62Tz2xqlH"
      },
      "source": [
        "#efficientnet 学習はエラーがでるので、モデル保存はパス\n",
        "#save_model_path = os.path.join(backup_dir, 'efficientnet_model.h5')\n",
        "#efficientnet_model.save(save_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79teynms59OB"
      },
      "source": [
        "save_model_path = os.path.join(backup_dir, 'my_model_aug.h5')\n",
        "model.save(save_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z3KsS8yfImf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6aKkRilxht1"
      },
      "source": [
        "#!cp './model/efficientnet_model.h5' '/content/drive/My Drive'\n",
        "!cp './model/my_model_aug.h5' '/content/drive/My Drive'\n",
        "!cp './model/labels.txt' '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T4GJRjlIwuB"
      },
      "source": [
        "# 参考リンク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qLRrlKiIyBB"
      },
      "source": [
        "- https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb\n",
        "- http://ni4muraano.hatenablog.com/entry/2019/06/16/084011\n",
        "- https://qiita.com/wakame1367/items/d90fa56bd9d11c4db50e"
      ]
    }
  ]
}